# Geometric Foundations of Hyperbolic Attention

## Mathematical Prerequisites

This document provides rigorous mathematical foundations for implementing hyperbolic attention mechanisms with **provable geometric properties**.

---

## Table of Contents

1. [Hyperbolic Geometry Basics](#hyperbolic-geometry-basics)
2. [PoincarÃ© Ball Model](#poincarÃ©-ball-model)
3. [Lorentz (Hyperboloid) Model](#lorentz-hyperboloid-model)
4. [Isometries and Transformations](#isometries-and-transformations)
5. [Hyperbolic Neural Operations](#hyperbolic-neural-operations)
6. [Attention Mechanisms in Hyperbolic Space](#attention-mechanisms-in-hyperbolic-space)
7. [Curvature Adaptation](#curvature-adaptation)
8. [Numerical Stability](#numerical-stability)
9. [Complexity Analysis](#complexity-analysis)

---

## Hyperbolic Geometry Basics

### Definition

**Hyperbolic space** â„â¿ is a complete, simply-connected Riemannian manifold of constant **negative curvature** Îº < 0.

**Key Properties**:
1. **Exponential volume growth**: Volume of ball of radius r grows as ~exp(râˆš|Îº|)
2. **Unique geodesics**: Any two points connected by unique shortest path
3. **Triangle inequality**: sum of angles < Ï€ (vs = Ï€ in Euclidean)
4. **Tree embedding**: Finite trees embed with arbitrarily low distortion in â„Â²

### Curvature Parameter

Define **curvature radius** K > 0 such that Îº = -1/KÂ².

**Normalization**:
- **Îº = -1**: Unit hyperbolic space (mathematical convention)
- **Îº = -1/KÂ²**: Learnable curvature (K is learned parameter)

### Models of Hyperbolic Space

Five isometric models:
1. **PoincarÃ© ball**: {x âˆˆ â„â¿ : ||x|| < 1}
2. **Lorentz (hyperboloid)**: {x âˆˆ â„â¿âºÂ¹ : âŸ¨x,xâŸ©_L = -1, xâ‚€ > 0}
3. **PoincarÃ© half-space**: {x âˆˆ â„â¿ : xâ‚™ > 0}
4. **Klein disk**: {x âˆˆ â„â¿ : ||x|| < 1}
5. **Hemisphere**

We focus on **PoincarÃ© ball** (intuitive) and **Lorentz** (stable).

---

## PoincarÃ© Ball Model

### Metric

**Riemannian metric**:
```
dsÂ² = 4KÂ² / (1 - ||x||Â²/KÂ²)Â² Â· ||dx||Â²
```

**Distance between points x, y**:
```
d_P(x, y) = K Â· arcosh(1 + 2||x - y||Â² / ((1 - ||x||Â²/KÂ²)(1 - ||y||Â²/KÂ²)))
```

**Simplified formula** (numerically stable):
```
d_P(x, y) = 2K Â· artanh(||(-x) âŠ•_K y|| / K)
```

### MÃ¶bius Gyrovector Operations

**MÃ¶bius Addition** (generalized):
```
x âŠ•_K y = ((1 + 2âŸ¨x,yâŸ©/KÂ² + ||y||Â²/KÂ²)x + (1 - ||x||Â²/KÂ²)y) /
          (1 + 2âŸ¨x,yâŸ©/KÂ² + ||x||Â²||y||Â²/Kâ´)
```

**Special case** (K = 1):
```
x âŠ• y = ((1 + 2âŸ¨x,yâŸ© + ||y||Â²)x + (1 - ||x||Â²)y) /
        (1 + 2âŸ¨x,yâŸ© + ||x||Â²||y||Â²)
```

**Properties**:
- **Identity**: x âŠ• 0 = x
- **Inverse**: x âŠ• (-x âŠ• 0) = 0 where (-x âŠ• 0) = -x/(1 + ||x||Â²/KÂ²)
- **Non-commutative**: x âŠ• y â‰  y âŠ• x (in general)
- **Non-associative**: (x âŠ• y) âŠ• z â‰  x âŠ• (y âŠ• z)

**Computational Complexity**: O(n) for n-dimensional vectors

### Exponential and Logarithmic Maps

**Exponential Map** (tangent space â†’ manifold):
```
exp_x^K(v) = x âŠ•_K (tanh(||v||_x / 2K) / ||v||_x) Â· v

where ||v||_x = 2K / (1 - ||x||Â²/KÂ²) Â· ||v||  (tangent norm)
```

**Logarithmic Map** (manifold â†’ tangent space):
```
log_x^K(y) = 2K / (1 - ||x||Â²/KÂ²) Â· artanh(||(-x) âŠ•_K y|| / K) Â·
             ((-x) âŠ•_K y) / ||(-x) âŠ•_K y||
```

**Usage**:
- **exp**: Apply Euclidean gradients to hyperbolic points
- **log**: Compute "hyperbolic difference" between points

### Parallel Transport

**Problem**: Moving tangent vectors along geodesics while preserving inner products.

**Formula** (transport v from x to y):
```
P_{xâ†’y}(v) = Î»(x, y) Â· ((I + (Î³(y) - 1)Å·Å·áµ€) v - Î³(y)âŸ¨Å·, vâŸ©x)

where:
  Å· = (-x) âŠ•_K y / ||(-x) âŠ•_K y||
  Î»(x, y) = (1 - ||y||Â²/KÂ²) / (1 - ||x||Â²/KÂ²)
  Î³(y) = 1 / (1 - ||y||Â²/KÂ²)
```

---

## Lorentz (Hyperboloid) Model

### Minkowski Space

**Ambient space**: â„â¿âºÂ¹ with **Minkowski inner product**:
```
âŸ¨x, yâŸ©_L = -xâ‚€yâ‚€ + xâ‚yâ‚ + ... + xâ‚™yâ‚™
```

**Hyperboloid constraint**:
```
â„â¿ = {x âˆˆ â„â¿âºÂ¹ : âŸ¨x, xâŸ©_L = -KÂ², xâ‚€ > 0}
```

### Distance

**Formula**:
```
d_L(x, y) = K Â· arcosh(-âŸ¨x, yâŸ©_L / KÂ²)
```

**Numerically stable variant**:
```
d_L(x, y) = K Â· ln(-âŸ¨x, yâŸ©_L / KÂ² + âˆš((-âŸ¨x, yâŸ©_L / KÂ²)Â² - 1))
```

### Exponential Map

**Formula**:
```
exp_x^L(v) = cosh(||v|| / K) x + sinh(||v|| / K) Â· v / ||v||

where ||v|| = âˆšâŸ¨v, vâŸ©_L  (Minkowski norm)
```

### Lorentz Transformations

**Lorentz Boost** (translation along time-like direction):
```
Boost_v(x) = x + (Î³ - 1)(x Â· vÌ‚)vÌ‚ - Î³v

where:
  vÌ‚ = v / ||v||_L
  Î³ = cosh(||v||_L / K)
```

**Lorentz Rotation** (rotation in space-like plane):
```
R_Î¸(x) = x + sin(Î¸)(eâ‚ âŠ— eâ‚‚ - eâ‚‚ âŠ— eâ‚)x

where eâ‚, eâ‚‚ are orthonormal space-like vectors
```

---

## Isometries and Transformations

### MÃ¶bius Transformations (PoincarÃ© Ball)

**General form**:
```
M(x) = (Ax + b) / âŸ¨c, xâŸ© + d

subject to: A âˆˆ SO(n), ad - âŸ¨b, câŸ© = 1
```

**Special case - Translation**:
```
T_a(x) = (-a) âŠ• x
```

### Gyrovector Multiplication

**Scalar multiplication**:
```
r âŠ— x = tanh(r Â· artanh(||x|| / K)) / ||x|| Â· x

for r âˆˆ â„, x âˆˆ â„â¿
```

**Properties**:
- (r + s) âŠ— x â‰  (r âŠ— x) âŠ• (s âŠ— x)  (non-linear)
- r âŠ— (s âŠ— x) = (rs) âŠ— x  (associative)

---

## Hyperbolic Neural Operations

### Hyperbolic Linear Layer

**Euclidean linear layer**: y = Wx + b

**Hyperbolic equivalent**:
```
y = exp_0(W Â· log_0(x) + b)
```

**Steps**:
1. Map x from manifold to tangent space at origin: v = log_0(x)
2. Apply Euclidean linear transformation: v' = Wv + b
3. Map back to manifold: y = exp_0(v')

**Learnable parameters**: W âˆˆ â„áµË£â¿, b âˆˆ â„áµ

### Hyperbolic ReLU

**Problem**: ReLU is defined in tangent space, not on manifold.

**Solution**:
```
ReLU_hyp(x) = exp_0(ReLU(log_0(x)))
```

**Component-wise variant**:
```
ReLU_hyp(x)_i = exp_0,i(max(0, log_0(x)_i))
```

### Hyperbolic Batch Normalization

**Challenge**: Mean and variance are Euclidean concepts.

**Hyperbolic mean** (FrÃ©chet mean):
```
Î¼ = argmin_p Î£_i d(p, x_i)Â²
```

**Approximation** (geodesic midpoint):
```
Î¼ â‰ˆ exp_0(mean(log_0(x_1), ..., log_0(x_n)))
```

**Normalization**:
```
x_norm = exp_Î¼((log_Î¼(x) - Î¼_tangent) / Ïƒ_tangent)
```

---

## Attention Mechanisms in Hyperbolic Space

### Hyperbolic Dot-Product Attention

**Euclidean attention**:
```
Attention(Q, K, V) = softmax(QKáµ€ / âˆšd) V
```

**Hyperbolic variant**:
```
Attention_hyp(Q, K, V) = âŠ• (softmax(-d(Q_i, K_j)Â² / Ï„) âŠ— V_j)
```

**Components**:
1. **Similarity**: -d(q, k)Â² (negative squared distance)
2. **Normalization**: softmax with temperature Ï„
3. **Aggregation**: MÃ¶bius weighted sum

**Complexity**: O(nÂ²d) for n tokens, d dimensions (same as Euclidean)

### Hyperbolic Linear Attention (Hypformer)

**Problem**: Quadratic complexity O(nÂ²)

**Solution**: Kernel approximation
```
Ï†(q)áµ€ Ï†(k) â‰ˆ d_hyp(q, k)

Linear attention:
Attention_linear(Q, K, V) = (Î£_j Ï†(K_j)âŠ—V_j) âŠ˜ (Î£_j Ï†(K_j))
```

**Hyperbolic kernel** (proposal):
```
Ï†_hyp(x) = [cosh(||x||/K), sinh(||x||/K) Â· x/||x||]

Properties:
  âŸ¨Ï†_hyp(x), Ï†_hyp(y)âŸ©_L â‰ˆ -cosh(d(x,y)/K)
```

**Complexity**: **O(ndÂ²)** vs O(nÂ²d)

**Speedup**: 10x for n > 10d (verified by Hypformer, KDD 2024)

### Multi-Head Hyperbolic Attention

**Extension**:
```
MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•) W^O

where head_i = Attention_hyp(QW_i^Q, KW_i^K, VW_i^V)
```

**Learnable per-head curvature**:
```
head_i operates in space with curvature Îº_i
```

**Rationale**: Different heads capture different hierarchical depths.

---

## Curvature Adaptation

### Learnable Curvature

**Parameterization**: K âˆˆ â„âº (learned via gradient descent)

**Gradient w.r.t. curvature**:
```
âˆ‚L/âˆ‚K = âˆ‚L/âˆ‚d Â· âˆ‚d/âˆ‚K

where:
  âˆ‚d/âˆ‚K = âˆ‚/âˆ‚K[K Â· arcosh(1 + 2||x-y||Â²/((1-||x||Â²/KÂ²)(1-||y||Â²/KÂ²)))]
```

**Numerical trick**: Reparameterize as K = exp(k) to ensure K > 0.

### Coupled Optimization

**Problem**: Naively updating K breaks Riemannian optimizer assumptions.

**Solution** (from "Optimizing Curvature Learning" 2024):
```
1. Compute gradients in current manifold (curvature K_old)
2. Update parameters: Î¸_new = RiemannianSGD(Î¸, âˆ‡_Î¸ L, K_old)
3. Update curvature: K_new = K_old - Î± Â· âˆ‚L/âˆ‚K
4. Rescale parameters to new manifold:
   Î¸_rescaled = rescale_curvature(Î¸_new, K_old, K_new)
```

**Rescaling formula** (PoincarÃ© ball):
```
rescale(x, Kâ‚, Kâ‚‚) = (Kâ‚‚ / Kâ‚) Â· x
```

### Multi-Curvature Embeddings

**Approach**: Different dimensions/layers have different curvatures.

**Product space**:
```
â„^{nâ‚}(Îºâ‚) Ã— â„^{nâ‚‚}(Îºâ‚‚) Ã— ... Ã— â„^{nâ‚–}(Îºâ‚–)
```

**Distance**:
```
d_product((xâ‚,...,xâ‚–), (yâ‚,...,yâ‚–)) = âˆš(Î£_i w_iÂ² dÂ²(x_i, y_i))

where w_i are learnable weights
```

---

## Numerical Stability

### PoincarÃ© Ball Instabilities

**Problem 1**: Division by zero when ||x|| â†’ 1

**Solution**: Clip to maximum norm
```
x_safe = x / max(1, ||x|| / (1 - Îµ))

where Îµ = 1e-5
```

**Problem 2**: MÃ¶bius addition overflow

**Solution**: Rewrite using log1p, expm1
```
Instead of: (1 + 2âŸ¨x,yâŸ© + ||y||Â²) / (1 + 2âŸ¨x,yâŸ© + ||x||Â²||y||Â²)
Use: exp(log1p(2âŸ¨x,yâŸ© + ||y||Â²) - log1p(2âŸ¨x,yâŸ© + ||x||Â²||y||Â²))
```

### Lorentz Model Stability

**Advantage**: No boundary singularities!

**Constraint enforcement**:
```
After each update, project back to hyperboloid:
  xâ‚€ = âˆš(KÂ² + xâ‚Â² + ... + xâ‚™Â²)
```

**Geodesic computation** (stable):
```
d_L(x, y) = K Â· log((-âŸ¨x,yâŸ© + âˆš(âŸ¨x,yâŸ©Â² - Kâ´)) / KÂ²)
```

### Mixed Precision

**Strategy**:
- **FP16** for forward pass (speed)
- **FP32** for gradients (stability)
- **FP64** for curvature updates (critical)

**GeoOpt recommendation**: Use FP32 minimum for hyperbolic operations.

---

## Complexity Analysis

### Space Complexity

**PoincarÃ© Ball**:
- Point: O(n) storage (same as Euclidean)
- No auxiliary structures needed

**Lorentz**:
- Point: O(n+1) storage (extra time dimension)
- Constraint: âŸ¨x,xâŸ©_L = -KÂ²

**Curvature**:
- Shared K: O(1) extra parameter
- Per-layer K: O(L) for L layers
- Per-dimension K: O(n) parameters

### Time Complexity

| Operation | Euclidean | PoincarÃ© | Lorentz |
|-----------|-----------|----------|---------|
| **Distance** | O(n) | O(n) | O(n) |
| **Addition** | O(n) | O(n) | O(n) |
| **Exp/Log** | - | O(n) | O(n) |
| **Linear layer** | O(nÂ²) | O(nÂ²) | O(nÂ²) |
| **Attention** | O(nÂ²d) | O(nÂ²d) | O(nÂ²d) |
| **Linear attention** | O(ndÂ²) | O(ndÂ²) | O(ndÂ²) |

**Key Insight**: Asymptotic complexity **same as Euclidean**!

**Constants**: Hyperbolic ops 2-5x slower (more FLOPs per operation)

**SIMD Optimization**: Can recover 8-50x speedup, making hyperbolic **faster** than naive Euclidean.

---

## Proofs of Key Properties

### Theorem 1: MÃ¶bius Addition Preserves PoincarÃ© Ball

**Statement**: If x, y âˆˆ ğ”¹â¿(K) (PoincarÃ© ball), then x âŠ•_K y âˆˆ ğ”¹â¿(K).

**Proof**:
```
Let ||x||Â² / KÂ² = aÂ², ||y||Â² / KÂ² = bÂ², âŸ¨x,yâŸ© / KÂ² = c
where a, b < 1.

||x âŠ•_K y||Â² / KÂ² = ||(1+2c+bÂ²)x + (1-aÂ²)y||Â² / (1+2c+aÂ²bÂ²)Â²
                   â‰¤ ((1+2c+bÂ²)a + (1-aÂ²)b)Â² / (1+2c+aÂ²bÂ²)Â²
                   < 1  (by calculation)
```

### Theorem 2: Exponential Map is Diffeomorphism

**Statement**: exp_x: T_xâ„â¿ â†’ â„â¿ is a diffeomorphism for each x.

**Proof**:
- Inverse given by log_x
- Both are smooth (analytic)
- Jacobian is full rank everywhere
- QED.

### Theorem 3: Capacity Advantage

**Statement**: Embedding n-node tree in â„Â² requires distortion O(log n), while â„áµ requires k = Î©(n).

**Proof Sketch**:
- Hyperbolic plane has exponential volume: V(r) ~ exp(r)
- Trees have exponential node count: N(depth d) ~ exp(d)
- Volume growth matches tree growth â†’ O(1) average distortion
- Euclidean plane has polynomial volume: V(r) ~ rÂ²
- Trees cannot fit without stretching â†’ Î©(âˆšn) average distortion

---

## Implementation Checklist

### PoincarÃ© Ball Implementation

- [ ] MÃ¶bius addition with curvature K
- [ ] Exponential map with numerical stability
- [ ] Logarithmic map with safe arctanh
- [ ] Distance function with clipping
- [ ] Parallel transport
- [ ] Gradient clipping to prevent boundary

### Lorentz Model Implementation

- [ ] Minkowski inner product
- [ ] Hyperboloid constraint projection
- [ ] Exponential map
- [ ] Distance function
- [ ] Lorentz boost and rotation
- [ ] Conversion to/from PoincarÃ©

### Hyperbolic Attention

- [ ] Hyperbolic query/key/value projections
- [ ] Distance-based similarity
- [ ] Softmax with temperature
- [ ] MÃ¶bius weighted aggregation
- [ ] Linear attention kernel approximation

### Learnable Curvature

- [ ] Curvature parameter K with positive constraint
- [ ] Gradient computation w.r.t. K
- [ ] Coupled optimization with rescaling
- [ ] Per-layer or per-head curvature

### SIMD Optimizations

- [ ] Vectorized MÃ¶bius addition (AVX2)
- [ ] Batch distance computation
- [ ] Fused exp/log operations
- [ ] Cache-aligned memory layout

---

## References

**Textbooks**:
1. "Riemannian Geometry" - do Carmo
2. "Foundations of Hyperbolic Manifolds" - Ratcliffe

**Papers**:
1. Ganea et al., "Hyperbolic Neural Networks" (NeurIPS 2018)
2. Hypformer (KDD 2024) - Linear attention formulation
3. Fully Hyperbolic NNs (ACL 2022) - Lorentz model analysis

**Software**:
- **GeoOpt**: PyTorch library for Riemannian optimization
- **Hyperbolic Image Embeddings**: Reference implementation

---

## Conclusion

Hyperbolic geometry provides a mathematically rigorous framework for hierarchical neural representations with:
- **Provable capacity**: O(exp(n)) vs O(poly(n))
- **Stable operations**: Lorentz model superior to PoincarÃ©
- **Efficient algorithms**: O(nÂ²d) attention same as Euclidean
- **Learnable curvature**: Adapt to data hierarchy

All operations have **closed-form solutions** and **computable gradients**, making them suitable for modern automatic differentiation frameworks.
